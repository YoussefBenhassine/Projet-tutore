# pdp_explainer.py
# =====================================================
# üìà Partial Dependence Plots (PDP) pour pr√©diction ESG avec clustering
# =====================================================

import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.inspection import PartialDependenceDisplay, partial_dependence
import seaborn as sns
from typing import Optional, List

# Configuration du style matplotlib
plt.style.use('default')
sns.set_palette("husl")


def render_pdp_analysis(model, trainer, model_name: str = "Model"):
    """
    Analyse PDP compl√®te pour un mod√®le de pr√©diction ESG avec clustering
    
    Parameters:
    -----------
    model : mod√®le entra√Æn√© (RandomForest, LightGBM, etc.)
    trainer : objet RegressionTrainer contenant X_train, X_test, y_train, y_test
    model_name : nom du mod√®le pour l'affichage
    """
    
    st.markdown(f"## üìà Analyse PDP - {model_name}")
    
    st.markdown("""
    Les **Partial Dependence Plots (PDP)** montrent l'effet marginal des variables sur la pr√©diction ESG.
    Cette analyse permet de comprendre comment chaque variable (y compris les clusters) influence le score ESG pr√©dit.
    """)
    
    # ============================
    # Pr√©paration des donn√©es
    # ============================
    X_train = trainer.X_train.copy()
    feature_names = X_train.columns.tolist()
    
    # Limiter pour performance
    max_samples = 500
    X_pdp = (
        X_train.sample(max_samples, random_state=42)
        if len(X_train) > max_samples
        else X_train
    )
    
    st.success(f"‚úÖ **{len(X_pdp)}** observations utilis√©es (sur {len(X_train)} totales)")
    
    # Identifier si les clusters sont pr√©sents
    cluster_features = [f for f in feature_names if 'cluster' in f.lower()]
    has_clusters = len(cluster_features) > 0
    
    if has_clusters:
        st.info(f"üß© **Clusters d√©tect√©s**: {', '.join(cluster_features)}")
    
    # ============================
    # Calcul de l'importance des features
    # ============================
    feature_importance = None
    top_features = []
    
    st.markdown("### üéØ Importance des Variables")
    
    try:
        if hasattr(model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'Feature': feature_names,
                'Importance': model.feature_importances_
            }).sort_values('Importance', ascending=False)
            
            # Limiter au nombre de features disponibles
            n_top = min(15, len(feature_importance))
            top_features = feature_importance.head(n_top)['Feature'].tolist()
            
            # Affichage de l'importance
            col1, col2 = st.columns([3, 2])
            
            with col1:
                st.markdown(f"**üìä Top {n_top} Features par Importance:**")
                
                # Highlight cluster features
                def highlight_clusters(row):
                    if any(cluster in row['Feature'].lower() for cluster in ['cluster']):
                        return ['background-color: #ffeb9c; font-weight: bold'] * len(row)
                    return [''] * len(row)
                
                styled_df = feature_importance.head(n_top).style.apply(highlight_clusters, axis=1).format({'Importance': '{:.4f}'})
                st.dataframe(styled_df, use_container_width=True, hide_index=True)
            
            with col2:
                top_feat_data = feature_importance.head(n_top)
                fig_imp = plt.figure(figsize=(7, 6))
                colors = ['#ff9999' if 'cluster' in f.lower() else '#66b3ff' 
                         for f in top_feat_data['Feature'].values[::-1]]
                
                plt.barh(
                    range(n_top), 
                    top_feat_data['Importance'].values[::-1],
                    color=colors
                )
                plt.yticks(
                    range(n_top), 
                    top_feat_data['Feature'].values[::-1],
                    fontsize=9
                )
                plt.xlabel('Importance', fontsize=10, fontweight='bold')
                plt.title(f'Top {n_top} Features\n(Rouge = Cluster)', fontsize=11, fontweight='bold')
                plt.tight_layout()
                st.pyplot(fig_imp)
                plt.close()
                
        else:
            st.info("‚ÑπÔ∏è L'importance des features n'est pas disponible pour ce mod√®le.")
            top_features = feature_names[:15]
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Impossible de calculer l'importance: {e}")
        top_features = feature_names[:15]
    
    st.divider()
    
    # ============================
    # Navigation par onglets
    # ============================
    tabs = st.tabs([
        "üìä PDP Univari√©",
        "üß© Impact des Clusters",
        "üîÑ PDP Multi-Variables",
        "üìà Analyse Comparative"
    ])
    
    # ============================
    # TAB 1: PDP Univari√©
    # ============================
    with tabs[0]:
        st.markdown("### üìä PDP Univari√© - Effet d'une Variable")
        
        st.markdown("""
        S√©lectionnez une variable pour voir son effet marginal sur la pr√©diction ESG.
        La courbe montre comment la pr√©diction varie en fonction de cette variable.
        """)
        
        selected_feature = st.selectbox(
            "üîç S√©lectionnez une variable:",
            options=feature_names,
            index=0 if not top_features else feature_names.index(top_features[0]),
            key="xai_pdp_univariate"
        )
        
        if selected_feature:
            _render_univariate_pdp(model, X_pdp, X_train, feature_names, selected_feature)
    
    # ============================
    # TAB 2: Impact des Clusters
    # ============================
    with tabs[1]:
        if has_clusters:
            st.markdown("### üß© Impact des Clusters sur la Pr√©diction ESG")
            
            st.markdown("""
            Cette section montre comment l'appartenance √† un cluster influence la pr√©diction du score ESG.
            Cela permet d'√©valuer si le clustering apporte une information pertinente au mod√®le.
            """)
            
            _render_cluster_impact(model, X_pdp, X_train, feature_names, cluster_features)
        else:
            st.warning("‚ö†Ô∏è Aucune variable de cluster d√©tect√©e dans le dataset.")
            st.info("üí° Assurez-vous que le clustering a √©t√© effectu√© et que la variable 'Cluster' est pr√©sente.")
    
    # ============================
    # TAB 3: PDP Multi-Variables
    # ============================
    with tabs[2]:
        st.markdown("### üîÑ PDP Multi-Variables - Comparaison")
        
        st.markdown("""
        Visualisez et comparez les PDP de plusieurs variables c√¥te √† c√¥te.
        """)
        
        _render_multivariate_pdp(model, X_pdp, feature_names, top_features)
    
    # ============================
    # TAB 4: Analyse Comparative
    # ============================
    with tabs[3]:
        st.markdown("### üìà Analyse Comparative des Top Features")
        
        if top_features:
            _render_comparative_analysis(model, X_pdp, feature_names, top_features, feature_importance)
        else:
            st.info("‚ÑπÔ∏è Analyse comparative non disponible.")


def _render_univariate_pdp(model, X_pdp, X_train, feature_names, selected_feature):
    """Rendu du PDP univari√©"""
    try:
        feature_idx = feature_names.index(selected_feature)
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            fig_pdp, ax_pdp = plt.subplots(figsize=(12, 6))
            
            display = PartialDependenceDisplay.from_estimator(
                model,
                X_pdp,
                features=[feature_idx],
                feature_names=feature_names,
                ax=ax_pdp,
                kind='both',
                grid_resolution=50,
                ice_lines_kw={'alpha': 0.1, 'linewidth': 0.5},
                pd_line_kw={'color': 'red', 'linewidth': 3}
            )
            
            ax_pdp.set_title(
                f'Partial Dependence Plot - {selected_feature}', 
                fontsize=15, 
                fontweight='bold',
                pad=20
            )
            ax_pdp.set_ylabel('Pr√©diction ESG', fontsize=12, fontweight='bold')
            ax_pdp.set_xlabel(selected_feature, fontsize=12, fontweight='bold')
            ax_pdp.grid(True, alpha=0.3)
            plt.tight_layout()
            st.pyplot(fig_pdp)
            plt.close()
        
        with col2:
            st.markdown("**üìä Statistiques**")
            st.metric("Min", f"{X_train[selected_feature].min():.3f}")
            st.metric("Max", f"{X_train[selected_feature].max():.3f}")
            st.metric("Moyenne", f"{X_train[selected_feature].mean():.3f}")
            st.metric("M√©diane", f"{X_train[selected_feature].median():.3f}")
            st.metric("√âcart-type", f"{X_train[selected_feature].std():.3f}")
        
        # Distribution
        st.markdown("**üìâ Distribution de la variable**")
        fig_dist, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))
        
        # Histogramme
        ax1.hist(X_train[selected_feature], bins=40, color='skyblue', edgecolor='black', alpha=0.7)
        ax1.set_xlabel(selected_feature, fontsize=10)
        ax1.set_ylabel('Fr√©quence', fontsize=10)
        ax1.set_title('Histogramme', fontsize=11, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # Box plot
        ax2.boxplot(X_train[selected_feature].dropna(), vert=False)
        ax2.set_xlabel(selected_feature, fontsize=10)
        ax2.set_title('Box Plot', fontsize=11, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig_dist)
        plt.close()
        
    except Exception as e:
        st.error(f"‚ùå Erreur: {e}")


def _render_cluster_impact(model, X_pdp, X_train, feature_names, cluster_features):
    """Rendu de l'impact des clusters"""
    
    for cluster_feat in cluster_features:
        st.markdown(f"#### üìä PDP - {cluster_feat}")
        
        try:
            feat_idx = feature_names.index(cluster_feat)
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                fig, ax = plt.subplots(figsize=(10, 6))
                
                display = PartialDependenceDisplay.from_estimator(
                    model,
                    X_pdp,
                    features=[feat_idx],
                    feature_names=feature_names,
                    ax=ax,
                    kind='both',
                    grid_resolution=30,
                    ice_lines_kw={'alpha': 0.15, 'linewidth': 0.8},
                    pd_line_kw={'color': 'darkred', 'linewidth': 4, 'label': 'PDP moyen'}
                )
                
                ax.set_title(
                    f'Impact du {cluster_feat} sur le Score ESG',
                    fontsize=14,
                    fontweight='bold',
                    pad=15
                )
                ax.set_ylabel('Pr√©diction ESG', fontsize=11, fontweight='bold')
                ax.set_xlabel(cluster_feat, fontsize=11, fontweight='bold')
                ax.legend()
                ax.grid(True, alpha=0.3)
                plt.tight_layout()
                st.pyplot(fig)
                plt.close()
            
            with col2:
                # Statistiques par cluster
                st.markdown("**üìà Distribution**")
                cluster_counts = X_train[cluster_feat].value_counts().sort_index()
                
                fig_bar, ax_bar = plt.subplots(figsize=(6, 4))
                cluster_counts.plot(kind='bar', ax=ax_bar, color='coral', edgecolor='black')
                ax_bar.set_title('Nb observations/cluster', fontsize=10, fontweight='bold')
                ax_bar.set_xlabel('Cluster', fontsize=9)
                ax_bar.set_ylabel('Count', fontsize=9)
                ax_bar.grid(True, alpha=0.3, axis='y')
                plt.xticks(rotation=0)
                plt.tight_layout()
                st.pyplot(fig_bar)
                plt.close()
                
                # Tableau
                st.dataframe(
                    cluster_counts.to_frame('Count'),
                    use_container_width=True
                )
            
            # Analyse de l'effet
            st.markdown("**üí° Interpr√©tation:**")
            
            # Calculer le PDP pour analyse
            pd_result = partial_dependence(
                model, 
                X_pdp, 
                features=[feat_idx],
                grid_resolution=30
            )
            
            avg_effect = pd_result['average'][0]
            effect_range = avg_effect.max() - avg_effect.min()
            
            if effect_range > 5:
                st.success(f"‚úÖ **Fort impact**: Le cluster a un effet important sur la pr√©diction ESG (variation de {effect_range:.2f} points)")
            elif effect_range > 2:
                st.info(f"‚ÑπÔ∏è **Impact mod√©r√©**: Le cluster influence moyennement la pr√©diction (variation de {effect_range:.2f} points)")
            else:
                st.warning(f"‚ö†Ô∏è **Faible impact**: Le cluster a peu d'effet sur la pr√©diction (variation de {effect_range:.2f} points)")
            
            st.divider()
            
        except Exception as e:
            st.error(f"‚ùå Erreur pour {cluster_feat}: {e}")


def _render_multivariate_pdp(model, X_pdp, feature_names, top_features):
    """Rendu du PDP multi-variables"""
    
    n_features_grid = st.slider(
        "üìä Nombre de variables √† afficher:",
        min_value=2,
        max_value=min(9, len(feature_names)),
        value=min(6, len(feature_names)),
        key="xai_multi_pdp_slider"
    )
    
    if top_features:
        default_features = top_features[:n_features_grid]
    else:
        default_features = feature_names[:n_features_grid]
    
    selected_features_grid = st.multiselect(
        "üîç S√©lectionnez les variables:",
        options=feature_names,
        default=default_features,
        key="xai_multi_pdp_select"
    )
    
    if len(selected_features_grid) >= 2:
        try:
            feature_indices = [feature_names.index(f) for f in selected_features_grid]
            
            n_features = len(feature_indices)
            n_cols = min(3, n_features)
            n_rows = (n_features + n_cols - 1) // n_cols
            
            fig_grid, axes = plt.subplots(
                n_rows, 
                n_cols, 
                figsize=(6 * n_cols, 4.5 * n_rows)
            )
            
            if n_rows == 1 and n_cols == 1:
                axes = np.array([[axes]])
            elif n_rows == 1:
                axes = axes.reshape(1, -1)
            elif n_cols == 1:
                axes = axes.reshape(-1, 1)
            
            for idx, feat_idx in enumerate(feature_indices):
                row = idx // n_cols
                col = idx % n_cols
                ax = axes[row, col]
                
                PartialDependenceDisplay.from_estimator(
                    model,
                    X_pdp,
                    features=[feat_idx],
                    feature_names=feature_names,
                    ax=ax,
                    kind='average',
                    grid_resolution=30,
                    pd_line_kw={'color': 'darkblue', 'linewidth': 2.5}
                )
                
                is_cluster = 'cluster' in feature_names[feat_idx].lower()
                color = 'darkred' if is_cluster else 'darkblue'
                
                ax.set_title(
                    f'{feature_names[feat_idx]}{"" if not is_cluster else " üß©"}', 
                    fontsize=12, 
                    fontweight='bold',
                    color=color
                )
                ax.set_ylabel('Pr√©diction ESG', fontsize=10)
                ax.grid(True, alpha=0.3)
            
            for idx in range(len(feature_indices), n_rows * n_cols):
                row = idx // n_cols
                col = idx % n_cols
                axes[row, col].axis('off')
            
            plt.tight_layout()
            st.pyplot(fig_grid)
            plt.close()
            
        except Exception as e:
            st.error(f"‚ùå Erreur: {e}")
    else:
        st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins 2 variables.")


def _render_bivariate_pdp(model, X_pdp, feature_names, top_features):
    """Rendu du PDP bivari√©"""
    
    col_a, col_b = st.columns(2)
    
    with col_a:
        feature_1 = st.selectbox(
            "üîπ Premi√®re variable:",
            options=feature_names,
            index=0 if not top_features else feature_names.index(top_features[0]),
            key="xai_pdp_2d_feat1"
        )
    
    with col_b:
        remaining_features = [f for f in feature_names if f != feature_1]
        default_idx_2 = 0
        if top_features and len(top_features) > 1:
            if top_features[1] != feature_1:
                default_idx_2 = remaining_features.index(top_features[1])
            elif len(top_features) > 2:
                default_idx_2 = remaining_features.index(top_features[2])
        
        feature_2 = st.selectbox(
            "üîπ Deuxi√®me variable:",
            options=remaining_features,
            index=default_idx_2,
            key="xai_pdp_2d_feat2"
        )
    
    if feature_1 and feature_2:
        try:
            feat_idx_1 = feature_names.index(feature_1)
            feat_idx_2 = feature_names.index(feature_2)
            
            fig_2d, ax_2d = plt.subplots(figsize=(12, 8))
            
            display_2d = PartialDependenceDisplay.from_estimator(
                model,
                X_pdp,
                features=[(feat_idx_1, feat_idx_2)],
                feature_names=feature_names,
                ax=ax_2d,
                kind='average',
                grid_resolution=25
            )
            
            ax_2d.set_title(
                f'PDP Bivari√©: {feature_1} √ó {feature_2}',
                fontsize=16,
                fontweight='bold',
                pad=20
            )
            
            plt.tight_layout()
            st.pyplot(fig_2d)
            plt.close()
            
            st.markdown("---")
            st.markdown("### üí° Interpr√©tation")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("**üîÜ Zones claires**")
                st.write("Pr√©dictions ESG **√©lev√©es**")
            with col2:
                st.markdown("**üåë Zones sombres**")
                st.write("Pr√©dictions ESG **faibles**")
            with col3:
                st.markdown("**üìê Contours**")
                st.write("Lignes d'interaction")
            
        except Exception as e:
            st.error(f"‚ùå Erreur: {e}")


def _render_ice_plots(model, X_pdp, feature_names, top_features):
    """Rendu des ICE plots"""
    
    ice_feature = st.selectbox(
        "üîç S√©lectionnez une variable:",
        options=feature_names,
        index=0 if not top_features else feature_names.index(top_features[0]),
        key="xai_ice_feature"
    )
    
    n_ice_samples = st.slider(
        "üìä Nombre d'observations:",
        min_value=10,
        max_value=min(100, len(X_pdp)),
        value=min(50, len(X_pdp)),
        step=10,
        key="xai_ice_samples"
    )
    
    if ice_feature:
        try:
            ice_idx = feature_names.index(ice_feature)
            X_ice = X_pdp.sample(n_ice_samples, random_state=42) if len(X_pdp) > n_ice_samples else X_pdp
            
            fig_ice, ax_ice = plt.subplots(figsize=(12, 7))
            
            display_ice = PartialDependenceDisplay.from_estimator(
                model,
                X_ice,
                features=[ice_idx],
                feature_names=feature_names,
                ax=ax_ice,
                kind='individual',
                grid_resolution=50,
                ice_lines_kw={'alpha': 0.3, 'linewidth': 0.8}
            )
            
            ax_ice.set_title(
                f'ICE Plots - {ice_feature} ({n_ice_samples} observations)',
                fontsize=15,
                fontweight='bold',
                pad=20
            )
            ax_ice.set_ylabel('Pr√©diction ESG', fontsize=12, fontweight='bold')
            ax_ice.set_xlabel(ice_feature, fontsize=12, fontweight='bold')
            ax_ice.grid(True, alpha=0.3)
            
            plt.tight_layout()
            st.pyplot(fig_ice)
            plt.close()
            
            st.markdown("---")
            st.markdown("### üí° Interpr√©tation des ICE Plots")
            
            col1, col2 = st.columns(2)
            with col1:
                st.success("""
                **‚úÖ Lignes parall√®les**
                - Effet homog√®ne de la variable
                - Comportement consistant du mod√®le
                - Pas d'interaction forte
                """)
            
            with col2:
                st.warning("""
                **‚ö†Ô∏è Lignes divergentes**
                - Effet h√©t√©rog√®ne
                - Pr√©sence d'interactions
                - Comportement contextuel
                """)
            
        except Exception as e:
            st.error(f"‚ùå Erreur: {e}")


def _render_comparative_analysis(model, X_pdp, feature_names, top_features, feature_importance):
    """Analyse comparative des top features"""
    
    st.markdown("""
    Cette section compare les courbes PDP normalis√©es des variables les plus importantes.
    Les pentes plus raides indiquent un effet plus fort sur la pr√©diction.
    """)
    
    n_top_compare = st.slider(
        "Nombre de features √† comparer:",
        min_value=3,
        max_value=min(10, len(top_features)),
        value=min(6, len(top_features)),
        key="xai_comparative_slider"
    )
    
    try:
        features_to_compare = top_features[:n_top_compare]
        
        # Calculer les effets PDP pour toutes les features
        pdp_data = []
        for feat_name in features_to_compare:
            feat_idx = feature_names.index(feat_name)
            pd_result = partial_dependence(model, X_pdp, features=[feat_idx], grid_resolution=30)
            avg = pd_result['average'][0]
            effect_range = avg.max() - avg.min()
            is_cluster = 'cluster' in feat_name.lower()
            
            pdp_data.append({
                'Feature': feat_name,
                'Effet PDP': effect_range,
                'Type': 'Cluster' if is_cluster else 'Variable'
            })
        
        pdp_df = pd.DataFrame(pdp_data).sort_values('Effet PDP', ascending=True)
        
        # Cr√©er un graphique √† barres horizontales plus lisible
        fig_compare, ax_compare = plt.subplots(figsize=(12, max(6, n_top_compare * 0.6)))
        
        colors = ['#ff6b6b' if t == 'Cluster' else '#4ecdc4' for t in pdp_df['Type']]
        
        bars = ax_compare.barh(
            range(len(pdp_df)), 
            pdp_df['Effet PDP'],
            color=colors,
            edgecolor='black',
            linewidth=1.5,
            alpha=0.8
        )
        
        # Ajouter les valeurs sur les barres
        for i, (idx, row) in enumerate(pdp_df.iterrows()):
            value = row['Effet PDP']
            ax_compare.text(
                value + 0.01 * pdp_df['Effet PDP'].max(), 
                i, 
                f'{value:.3f}',
                va='center',
                fontsize=10,
                fontweight='bold'
            )
        
        ax_compare.set_yticks(range(len(pdp_df)))
        ax_compare.set_yticklabels(
            [f"{'üß© ' if t == 'Cluster' else 'üìä '}{f}" for f, t in zip(pdp_df['Feature'], pdp_df['Type'])],
            fontsize=11
        )
        ax_compare.set_xlabel('Effet PDP (variation de pr√©diction)', fontsize=12, fontweight='bold')
        ax_compare.set_title(
            f'Impact des Top {n_top_compare} Features sur la Pr√©diction ESG',
            fontsize=14,
            fontweight='bold',
            pad=15
        )
        ax_compare.grid(True, alpha=0.3, axis='x')
        
        # L√©gende
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='#4ecdc4', edgecolor='black', label='üìä Variable'),
            Patch(facecolor='#ff6b6b', edgecolor='black', label='üß© Cluster')
        ]
        ax_compare.legend(handles=legend_elements, loc='lower right', fontsize=10)
        
        plt.tight_layout()
        st.pyplot(fig_compare)
        plt.close()
        
        # Tableau r√©capitulatif
        st.markdown("### üìã R√©sum√© de l'Impact")
        
        if feature_importance is not None:
            summary_data = []
            for feat_name in features_to_compare:
                feat_idx = feature_names.index(feat_name)
                pd_result = partial_dependence(model, X_pdp, features=[feat_idx], grid_resolution=30)
                avg = pd_result['average'][0]
                effect_range = avg.max() - avg.min()
                importance = feature_importance[feature_importance['Feature'] == feat_name]['Importance'].values[0]
                
                # Calculer le pourcentage d'importance
                total_importance = feature_importance['Importance'].sum()
                importance_pct = (importance / total_importance) * 100
                
                summary_data.append({
                    'Variable': feat_name,
                    'Importance Mod√®le (%)': importance_pct,
                    'Effet PDP': effect_range,
                    'Type': 'üß© Cluster' if 'cluster' in feat_name.lower() else 'üìä Variable'
                })
            
            summary_df = pd.DataFrame(summary_data).sort_values('Importance Mod√®le (%)', ascending=False)
            
            st.dataframe(
                summary_df.style.format({
                    'Importance Mod√®le (%)': '{:.2f}%',
                    'Effet PDP': '{:.3f}'
                }).background_gradient(subset=['Importance Mod√®le (%)', 'Effet PDP'], cmap='YlOrRd'),
                use_container_width=True,
                hide_index=True
            )
            
            st.info("""
            ‚ÑπÔ∏è **Interpr√©tation du tableau** :
            - **Importance Mod√®le (%)** : Contribution de chaque variable dans les d√©cisions du mod√®le (100% au total)
            - **Effet PDP** : Variation r√©elle de la pr√©diction ESG caus√©e par cette variable
            - Plus le % est √©lev√©, plus le mod√®le utilise cette variable pour pr√©dire
            - Plus l'Effet PDP est grand, plus la variable change le score ESG pr√©dit
            """)
        else:
            st.warning("‚ö†Ô∏è L'importance des features n'est pas disponible pour ce mod√®le.")
            
    except Exception as e:
        st.error(f"‚ùå Erreur: {e}")


# ============================
# Application standalone
# ============================
def main():
    """Application standalone pour tester le module PDP"""
    st.set_page_config(
        page_title="PDP Explainer - ESG Prediction",
        page_icon="üìà",
        layout="wide"
    )
    
    st.title("üìà PDP Explainer - Pr√©diction ESG avec Clustering")
    st.markdown("---")
    
    st.warning("""
    ‚ö†Ô∏è **Module d'explainabilit√© PDP**
    
    Ce module n√©cessite:
    1. Un mod√®le de pr√©diction entra√Æn√© (RandomForest, LightGBM, etc.)
    2. Des donn√©es d'entra√Ænement avec les clusters
    3. L'objet `RegressionTrainer` contenant X_train, X_test, etc.
    
    **Utilisation**: Int√©grer dans l'application principale via `render_pdp_analysis(model, trainer, model_name)`
    """)
    
    st.info("""
    **üìö √Ä propos des Partial Dependence Plots:**
    
    Les PDP montrent l'effet marginal des variables sur les pr√©dictions. Dans le contexte ESG avec clustering:
    - **Variables standard**: Montrent leur impact direct sur le score ESG
    - **Variables de cluster**: R√©v√®lent si le regroupement apporte de l'information pr√©dictive
    - **Interactions**: Identifient les synergies entre variables
    
    **Avantages**:
    ‚úÖ Visualisation intuitive
    ‚úÖ D√©tection des non-lin√©arit√©s
    ‚úÖ √âvaluation de l'impact du clustering
    ‚úÖ Ind√©pendant du type de mod√®le
    """)


if __name__ == "__main__":
    main()
