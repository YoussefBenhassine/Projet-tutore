# ============================================================
# PHASE DE PR√âTRAITEMENT DES DONN√âES ESG
# ============================================================

# ============================================================
# 1. IMPORTATION DES LIBRAIRIES
# ============================================================
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel
import xgboost as xgb

# ============================================================
# 2. FONCTION DE DIAGNOSTIC OVERFITTING
# ============================================================
def overfitting_check(model_name, y_train, y_train_pred, y_test, y_test_pred):
    r2_train = r2_score(y_train, y_train_pred)
    r2_test = r2_score(y_test, y_test_pred)
    print(f"\nDiagnostic {model_name}:")
    print(f"R¬≤ Train: {r2_train:.4f}")
    print(f"R¬≤ Test : {r2_test:.4f}")
    if r2_train - r2_test > 0.1:
        print("‚ö†Ô∏è Attention : Risque de surapprentissage d√©tect√© !")
    else:
        print("‚úÖ Pas de surapprentissage significatif.")

# ============================================================
# 3. CHARGEMENT DATASET ORIGINAL
# ============================================================
import os
dataset_path = "data/esg_dataset.csv"
if not os.path.exists(dataset_path):
    dataset_path = "esg_dataset.csv"
df = pd.read_csv(dataset_path)

# Colonnes discr√®tes (cat√©gorielles)
categorical_columns = ['Sector']

# ============================================================
# 4. NETTOYAGE & ENCODAGE
# ============================================================
# Convertir les colonnes non num√©riques
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Supprimer les lignes o√π ESG_Score est manquant
df = df.dropna(subset=['ESG_Score'])

# One-hot encoding des colonnes cat√©gorielles
df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)
=======
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel
import os

# ============================================================
# 2. CHARGEMENT DU DATASET
# ============================================================
dataset_path = "data/esg_dataset.csv"
if not os.path.exists(dataset_path):
    dataset_path = "esg_dataset.csv"

df = pd.read_csv(dataset_path)

# Colonnes cat√©gorielles
categorical_columns = ['Sector']

# ============================================================
# 3. NETTOYAGE DES DONN√âES
# ============================================================

# Conversion des colonnes non num√©riques
# üî¥ MODIFICATION UNIQUE ICI (Sector est exclu)
for col in df.columns:
    if df[col].dtype == 'object' and col not in categorical_columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Suppression des lignes o√π ESG_Score est manquant
df = df.dropna(subset=['ESG_Score'])

# Encodage One-Hot des variables cat√©gorielles
df = pd.get_dummies(df, columns=categorical_columns)
>>>>>>> pdp-explainability

print("Colonnes apr√®s nettoyage :", df.columns)

# ============================================================
# 4b. DETECTION & TRAITEMENT DES OUTLIERS
# ============================================================
numeric_features = df.select_dtypes(include=np.number).columns.tolist()
numeric_features.remove('ESG_Score')
if 'Company_ID' in numeric_features:
    numeric_features.remove('Company_ID')

for col in numeric_features:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])

print("‚úÖ Outliers trait√©s pour les features num√©riques.")

# ============================================================
# 5. SELECTION DES FEATURES
# ============================================================
features = df.select_dtypes(include=np.number).columns.tolist()
if 'Company_ID' in features:
    features.remove('Company_ID')
features.remove('ESG_Score')

X = df[features]
y = df['ESG_Score']

# Standardisation
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# S√©lection via RandomForest
selector = SelectFromModel(RandomForestRegressor(n_estimators=200, random_state=42), threshold="median")
selector.fit(X_scaled, y)
selected_features = X.columns[selector.get_support()]
X_selected = X[selected_features]

print("Features s√©lectionn√©es :", selected_features)
<<<<<<< HEAD

# ============================================================
# 6. TRAIN / TEST SPLIT
# ============================================================
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# ============================================================
# 7. RANDOM FOREST (r√©duction overfitting)
# ============================================================
rf = RandomForestRegressor(random_state=42)
rf_params = {
    'n_estimators': [200, 250],
    'max_depth': [2, 3],
    'min_samples_split': [15, 20],
    'min_samples_leaf': [10, 15],
    'max_features': ['sqrt']
}
rf_random = RandomizedSearchCV(rf, rf_params, n_iter=15, cv=5, scoring='r2', random_state=42, n_jobs=-1)
rf_random.fit(X_train, y_train)
best_rf = rf_random.best_estimator_

rf_train_preds = best_rf.predict(X_train)
rf_preds = best_rf.predict(X_test)

rmse_rf = np.sqrt(mean_squared_error(y_test, rf_preds))
r2_rf = r2_score(y_test, rf_preds)

print("\nRandom Forest -> RMSE:", rmse_rf, ", R¬≤:", r2_rf)
print("Random Forest CV R¬≤ moyen:", rf_random.best_score_)

# ============================================================
# 8. XGBOOST (Early Stopping et r√©gularisation)
# ============================================================
X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
dtrain = xgb.DMatrix(X_train_final, label=y_train_final)
dval = xgb.DMatrix(X_val, label=y_val)
dtest = xgb.DMatrix(X_test, label=y_test)

final_params = {
    'objective': 'reg:squarederror',
    'learning_rate': 0.01,
    'max_depth': 2,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'gamma': 0.2,
    'reg_alpha': 10.0,
    'reg_lambda': 10.0,
    'seed': 42
}

final_xgb = xgb.train(
    params=final_params,
    dtrain=dtrain,
    num_boost_round=5000,
    evals=[(dval, 'validation')],
    early_stopping_rounds=50,
    verbose_eval=False
)

xgb_train_preds = final_xgb.predict(dtrain)
xgb_preds = final_xgb.predict(dtest)

rmse_xgb = np.sqrt(mean_squared_error(y_test, xgb_preds))
r2_xgb = r2_score(y_test, xgb_preds)

print("\nXGBoost -> RMSE:", rmse_xgb, ", R¬≤:", r2_xgb)

# ============================================================
# 9. Comparaison des pr√©dictions
# ============================================================
comparison = pd.DataFrame({
    'ESG_Reel': y_test.values,
    'RF_Predit': rf_preds,
    'XGB_Predit': xgb_preds
})
print("\nComparaison des pr√©dictions :")
print(comparison.head())

# ============================================================
# 10. Diagnostic surapprentissage
# ============================================================
overfitting_check("Random Forest", y_train, rf_train_preds, y_test, rf_preds)
overfitting_check("XGBoost", y_train_final, xgb_train_preds, y_test, xgb_preds)

# ============================================================
# 11. CALCUL DES M√âTRIQUES POUR COMPARAISON
# ============================================================
from sklearn.metrics import mean_absolute_error

def calculate_all_metrics(y_true, y_pred, model_name):
    """Calcule toutes les m√©triques de r√©gression."""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    # MAPE avec gestion des valeurs nulles (calcul manuel)
    mask = y_true != 0
    if np.sum(mask) > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        mape = np.nan
    
    return {
        'Model': model_name,
        'R2_Score': r2,
        'RMSE': rmse,
        'MAE': mae,
        'MAPE': mape
    }

# Calcul des m√©triques pour Random Forest
rf_metrics = calculate_all_metrics(y_test, rf_preds, 'Random Forest (Sans Clustering)')
rf_train_metrics = calculate_all_metrics(y_train, rf_train_preds, 'Random Forest Train (Sans Clustering)')

# Calcul des m√©triques pour XGBoost
xgb_metrics = calculate_all_metrics(y_test, xgb_preds, 'XGBoost (Sans Clustering)')
xgb_train_metrics = calculate_all_metrics(y_train_final, xgb_train_preds, 'XGBoost Train (Sans Clustering)')

# Cr√©ation d'un DataFrame avec toutes les m√©triques
metrics_df = pd.DataFrame([
    rf_metrics,
    xgb_metrics
])

# Sauvegarde des m√©triques
os.makedirs("prediction_sans_cls", exist_ok=True)
metrics_df.to_csv("prediction_sans_cls/metrics_sans_clustering.csv", index=False)
print("\n‚úÖ M√©triques sauvegard√©es dans 'prediction_sans_cls/metrics_sans_clustering.csv'")

# Sauvegarde des pr√©dictions pour comparaison
predictions_df = pd.DataFrame({
    'True_Value': y_test.values,
    'RF_Predicted': rf_preds,
    'XGB_Predicted': xgb_preds,
    'RF_Residual': y_test.values - rf_preds,
    'XGB_Residual': y_test.values - xgb_preds,
    'RF_Absolute_Error': np.abs(y_test.values - rf_preds),
    'XGB_Absolute_Error': np.abs(y_test.values - xgb_preds)
})
predictions_df.to_csv("prediction_sans_cls/predictions_sans_clustering.csv", index=False)
print("‚úÖ Pr√©dictions sauvegard√©es dans 'prediction_sans_cls/predictions_sans_clustering.csv'")

# Affichage des m√©triques
print("\n" + "="*60)
print("M√âTRIQUES FINALES (SANS CLUSTERING)")
print("="*60)
print("\nRandom Forest:")
print(f"  R¬≤ Score: {rf_metrics['R2_Score']:.4f}")
print(f"  RMSE: {rf_metrics['RMSE']:.4f}")
print(f"  MAE: {rf_metrics['MAE']:.4f}")
print(f"  MAPE: {rf_metrics['MAPE']:.2f}%")

print("\nXGBoost:")
print(f"  R¬≤ Score: {xgb_metrics['R2_Score']:.4f}")
print(f"  RMSE: {xgb_metrics['RMSE']:.4f}")
print(f"  MAE: {xgb_metrics['MAE']:.4f}")
print(f"  MAPE: {xgb_metrics['MAPE']:.2f}%")
print("="*60)
